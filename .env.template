# Copy this file to .env and fill in your values

# LLM Provider API Keys
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_neo4j_password_here

# Neo4j MCP Server Configuration
# If set, use local Neo4j MCP server path instead of uvx
LOCAL_NEO4J_MCP_SERVER_PATH=

# Additional MCP Server Environment Variables (add as needed based on mcp_servers.json)
# Example for app-mod-neo4j server:
# APP_MOD_NEO4J_URI=neo4j://localhost:7688
# APP_MOD_NEO4J_USERNAME=neo4j
# APP_MOD_NEO4J_PASSWORD=your_app_mod_password

# Example for web search server:
# WEB_SEARCH_API_KEY=your_web_search_api_key

# Example for GitHub server:
# GITHUB_TOKEN=your_github_token

# Example for filesystem server:
# MCP_FILESYSTEM_ALLOWED_PATHS=./,/path/to/allowed/directory

# Atlas Configuration
ATLAS_LOG_LEVEL=INFO
ATLAS_OUTPUT_DIR=./output

# IF you wish to use LangChain and LangSmith Tracing then set these values.
# The analyze command tracing may prove useful in analysing LLM interactions.
# Ensure you have the correct values for your LangSmith project
LANGSMITH_TRACING=True
LANGCHAIN_TRACING_V2=true
LANGSMITH_PROJECT=your_project_name_here
LANGSMITH_API_KEY=your_langsmith_api_key_here
# Set the langsmith endpoint if you are using a custom region
LANGSMITH_ENDPOINT=https://api.langsmith.com

