# Copy this file to .env and fill in your values

# LLM Provider API Keys
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_neo4j_password_here

# Neo4j MCP Server Configuration
# If set, use local Neo4j MCP server path instead of uvx
LOCAL_NEO4J_MCP_SERVER_PATH=

# Atlas Configuration
ATLAS_LOG_LEVEL=INFO
ATLAS_OUTPUT_DIR=./output

# IF you wish to use LangChain and LangSmith Tracing then set these values.
# The analyze command tracing may prove useful in analysing LLM interactions.
# Ensure you have the correct values for your LangSmith project
LANGSMITH_TRACING=True
LANGCHAIN_TRACING_V2=true
LANGSMITH_PROJECT=your_project_name_here
LANGSMITH_API_KEY=your_langsmith_api_key_here
# Set the langsmith endpoint if you are using a custom region
LANGSMITH_ENDPOINT=https://api.langsmith.com

